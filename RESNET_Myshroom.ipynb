{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MyShroomBot/MyShroomBot/blob/main/RESNET_Myshroom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3tpGkVR_rUf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnVzevkRYHwQ",
        "outputId": "05d89383-0827-4fc3-8cae-823b27031587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory"
      ],
      "metadata": {
        "id": "_udL9J9RG6Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import io\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "import zipfile\n",
        "import PIL"
      ],
      "metadata": {
        "id": "grnNKJfkGzxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id7A8j5R_rUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04e60f8-3e78-4df6-fd53-3192cd3157bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 101MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the pretrained ResNet34 model\n",
        "resnet34 = models.resnet34(weights='IMAGENET1K_V1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#load resnet to gpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet34.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdfbVYA_ieZB",
        "outputId": "41362b81-259b-41e1-92c0-47b95531e75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((250,250)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((250,250)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "S6O_FlWnkr37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ZipDataset(Dataset):\n",
        "    def __init__(self, root_path, var,trns, cache_into_memory=False):\n",
        "        if cache_into_memory:\n",
        "            f = open(root_path, 'rb')\n",
        "            self.zip_content = f.read()\n",
        "            f.close()\n",
        "            self.zip_file = zipfile.ZipFile(io.BytesIO(self.zip_content), 'r')\n",
        "        else:\n",
        "            self.zip_file = zipfile.ZipFile(root_path, 'r')\n",
        "        self.name_list = list(filter(lambda x: var in x and (x[-4:] == '.jpg'), self.zip_file.namelist()))\n",
        "        species = list(filter(lambda x: x is not None,[filename.split('/')[2] if len(filename.split('/'))>3 else None for filename in self.name_list]))\n",
        "        self.species_dict={name:iter for iter,name in enumerate(np.unique(species))}\n",
        "        self.to_tensor = ToTensor()\n",
        "        self.transf = trns[var]\n",
        "    def __getitem__(self, key):\n",
        "      buf = self.zip_file.read(name=self.name_list[key])\n",
        "      img = cv2.imdecode(np.frombuffer(buf, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
        "      img = PIL.Image.fromarray(img)\n",
        "      target = self.species_dict[self.name_list[key].split(\"/\")[2]]\n",
        "      img = self.transf(img)\n",
        "      return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.name_list)"
      ],
      "metadata": {
        "id": "yOOT_XNCGrqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path1= '/content/drive/MyDrive/OG_best_dataset.zip'\n",
        "ZipDataset(root_path=path1, var='train', trns=data_transforms).species_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "EU16RF59fCV1",
        "outputId": "404a17ee-4d47-49b6-c41c-40e49bf5cff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ZipDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-946e834ac935>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/OG_best_dataset.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mZipDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecies_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ZipDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path1= '/content/drive/MyDrive/OG_best_dataset.zip'\n",
        "dataset_train = ZipDataset(root_path = path1,var='train', trns = data_transforms, cache_into_memory=False)\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=256, shuffle=True)\n",
        "dataset_val = ZipDataset(path1,var='val',trns = data_transforms,cache_into_memory=False)\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=256, shuffle=True)\n",
        "image_datasets = {'train': dataset_train, 'val': dataset_val}\n",
        "dataloaders = {'train': dataloader_train, 'val': dataloader_val}\n",
        "dataset_sizes = {'train': len(image_datasets['train']),'val': len(image_datasets['val'])}\n",
        "print(dataset_sizes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "9kskGbUAKboA",
        "outputId": "871daa85-1c72-4df3-97a7-5a8412578313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ZipDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0716b7dd118c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/OG_best_dataset.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_into_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataloader_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_into_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataloader_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ZipDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def imshow(inp, title=None):\n",
        "#     \"\"\"Display image for Tensor.\"\"\"\n",
        "#     inp = inp.numpy().transpose((1, 2, 0))\n",
        "#     mean = np.array([0.485, 0.456, 0.406])\n",
        "#     std = np.array([0.229, 0.224, 0.225])\n",
        "#     inp = std * inp + mean\n",
        "#     inp = np.clip(inp, 0, 1)\n",
        "#     plt.imshow(inp)\n",
        "#     if title is not None:\n",
        "#         plt.title(title)\n",
        "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# # Get a batch of training data\n",
        "# inputs, classes = next(iter(dataloaders['train']))\n",
        "# print(classes)\n",
        "\n",
        "# # Make a grid from batch\n",
        "# out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "# imshow(out, title=[dataset_train.species_dict[x] for x in classes])\n"
      ],
      "metadata": {
        "id": "OC9Lz4969feP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = dataset_train.species_dict\n",
        "print(class_names)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOXrhaXEKdZJ",
        "outputId": "9e2976a3-a18c-4c16-90ea-8a8837aa8701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Agaricus_augustus': 0, 'Agaricus_campestris': 1, 'Agaricus_xanthodermus': 2, 'Amanita_augusta': 3, 'Amanita_bisporigera': 4, 'Amanita_calyptroderma': 5, 'Amanita_citrina': 6, 'Amanita_flavoconia': 7, 'Amanita_flavorubens': 8, 'Amanita_fulva': 9, 'Amanita_jacksonii': 10, 'Amanita_junquillea': 11, 'Amanita_novinupta': 12, 'Amanita_pantherina': 13, 'Amanita_parcivolvata': 14, 'Amanita_persicina': 15, 'Amanita_phalloides': 16, 'Amanita_rubescens': 17, 'Amanita_vaginata': 18, 'Amanita_velosa': 19, 'Amanita_xanthocephala': 20, 'Aureoboletus_betula': 21, 'Aureoboletus_mirabilis': 22, 'Aureoboletus_russellii': 23, 'Baorangia_bicolor': 24, 'Boletus_edulis': 25, 'Boletus_reticulatus': 26, 'Boletus_rubriceps': 27, 'Cantharellus_californicus': 28, 'Cantharellus_cibarius': 29, 'Cantharellus_cinnabarinus': 30, 'Cantharellus_formosus': 31, 'Cantharellus_lateritius': 32, 'Cantharellus_minor': 33, 'Cerioporus_squamosus': 34, 'Chalciporus_piperatus': 35, 'Chlorophyllum_brunneum': 36, 'Chlorophyllum_molybdites': 37, 'Chlorophyllum_olivieri': 38, 'Chlorophyllum_rhacodes': 39, 'Clavulina_coralloides': 40, 'Clavulina_rugosa': 41, 'Craterellus_cornucopioides': 42, 'Craterellus_fallax': 43, 'Craterellus_tubaeformis': 44, 'Cuphophyllus_pratensis': 45, 'Cuphophyllus_virgineus': 46, 'Exsudoporus_frostii': 47, 'Fomes_excavatus': 48, 'Fomes_fomentarius': 49, 'Fomitopsis_betulina': 50, 'Fomitopsis_mounceae': 51, 'Fomitopsis_pinicola': 52, 'Ganoderma_applanatum': 53, 'Ganoderma_tsugae': 54, 'Gliophorus_psittacinus': 55, 'Grifola_frondosa': 56, 'Harrya_chromipes': 57, 'Hericium_americanum': 58, 'Hericium_coralloides': 59, 'Hericium_erinaceus': 60, 'Hortiboletus_rubellus': 61, 'Humidicutis_marginata': 62, 'Hydnum_repandum': 63, 'Hygrocybe_acutoconica': 64, 'Hygrocybe_cantharellus': 65, 'Hygrocybe_coccinea': 66, 'Hygrocybe_conica': 67, 'Hygrocybe_flavescens': 68, 'Hygrocybe_miniata': 69, 'Hygrocybe_singeri': 70, 'Imleria_badia': 71, 'Ischnoderma_resinosum': 72, 'Laetiporus_cincinnatus': 73, 'Laetiporus_gilbertsonii': 74, 'Laetiporus_sulphureus': 75, 'Leccinum_albostipitatum': 76, 'Leccinum_aurantiacum': 77, 'Leccinum_scabrum': 78, 'Leccinum_versipelle': 79, 'Leucoagaricus_americanus': 80, 'Leucoagaricus_leucothites': 81, 'Leucocoprinus_birnbaumii': 82, 'Leucocoprinus_cepistipes': 83, 'Leucocoprinus_fragilissimus': 84, 'Lichenomphalia_chromacea': 85, 'Lichenomphalia_umbellifera': 86, 'Macrolepiota_clelandii': 87, 'Macrolepiota_procera': 88, 'Meripilus_giganteus': 89, 'Meripilus_sumstinei': 90, 'Merulius_tremellosus': 91, 'Multiclavula_mucida': 92, 'Neoboletus_erythropus': 93, 'Neoboletus_luridiformis': 94, 'Other_agaricus_Species': 95, 'Other_agrocybe_Species': 96, 'Other_amanita_Species': 97, 'Other_bolete_Species': 98, 'Other_cantharellus_Species': 99, 'Other_oyster_Species': 100, 'Other_shelf_fungi_Species': 101, 'Phaeolus_schweinitzii': 102, 'Pleurotus_citrinopileatus': 103, 'Pleurotus_levis': 104, 'Pleurotus_ostreatus': 105, 'Pleurotus_pulmonarius': 106, 'Podaxis_pistillaris': 107, 'Psilocybe_cubensis': 108, 'Psilocybe_cyanescens': 109, 'Psilocybe_ovoideocystidiata': 110, 'Retiboletus_ornatipes': 111, 'Sparassis_crispa': 112, 'Strobilomyces_strobilaceus': 113, 'Suillellus_amygdalinus': 114, 'Suillellus_luridus': 115, 'Trametes_betulina': 116, 'Trametes_cinnabarina': 117, 'Trametes_coccinea': 118, 'Trametes_gibbosa': 119, 'Trametes_versicolor': 120, 'Tylopilus_felleus': 121, 'Tylopilus_plumbeoviolaceus': 122, 'Tylopilus_rubrobrunneus': 123, 'Xerocomellus_chrysenteron': 124, 'Xerocomellus_dryophilus': 125, 'Xerocomus_subtomentosus': 126}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def imshow(inp, title=None):\n",
        "#     \"\"\"Display image for Tensor.\"\"\"\n",
        "#     inp = inp.numpy().transpose((1, 2, 0))\n",
        "#     mean = np.array([0.485, 0.456, 0.406])\n",
        "#     # std = np.array([0.229, 0.224, 0.225])\n",
        "#     inp = std * inp + mean\n",
        "#     inp = np.clip(inp, 0, 1)\n",
        "#     plt.imshow(inp)\n",
        "#     if title is not None:\n",
        "#         plt.title(title)\n",
        "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# # Get a batch of training data\n",
        "# inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# # Make a grid from batch\n",
        "# out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "# imshow(out, title=[class_names[x] for x in classes])"
      ],
      "metadata": {
        "id": "SxQd7j8OBp3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT1GmeFR_rUk"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Model conditions\n",
        "new_model = resnet34\n",
        "num_classes = 127  # Suposición\n",
        "in_features = new_model.fc.in_features\n",
        "new_model.fc = nn.Linear(in_features, num_classes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BCUZBez_rUl"
      },
      "outputs": [],
      "source": [
        "#setup optimizers\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(new_model.parameters(), lr=0.001, momentum=0.9)\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer,\n",
        "                        milestones=[8, 24, 28], # List of epoch indices\n",
        "                        gamma =0.5) # Multiplicative factor of learning rate decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0x9cLul_rUl"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=4):\n",
        "    since = time.time()\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    with TemporaryDirectory() as tempdir:\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in tqdm(dataloaders[phase],leave=False):\n",
        "                    try:\n",
        "                      inputs = inputs.to(device)\n",
        "                      labels = labels.to(device)\n",
        "\n",
        "                      # zero the parameter gradients\n",
        "                      optimizer.zero_grad()\n",
        "\n",
        "                      # forward\n",
        "                      # track history if only in train\n",
        "                      with torch.set_grad_enabled(phase == 'train'):\n",
        "                          outputs = model(inputs)\n",
        "                          _, preds = torch.max(outputs, 1)\n",
        "                          loss = criterion(outputs, labels)\n",
        "\n",
        "                          # backward + optimize only if in training phase\n",
        "                          if phase == 'train':\n",
        "                              loss.backward()\n",
        "                              optimizer.step()\n",
        "                    except Exception as e:\n",
        "                      pass\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                if phase == 'train':\n",
        "                    scheduler.step()\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "        # load best model weights\n",
        "\n",
        "        model.load_state_dict(torch.load(best_model_params_path))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.load_state_dict(torch.load('/content/drive/MyDrive/saved_parameters/myshroomweights127final.pth'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqoSMFhryzIj",
        "outputId": "fefa3d50-4416-4d77-de5d-1929b1cc8ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "param_path='/content/drive/MyDrive/saved_parameters/muhsroomIDweights.pth'\n",
        "new_model.to(device)\n",
        "myshroom_model = train_model(new_model,criterion,optimizer,scheduler)\n",
        "torch.save(myshroom_model.state_dict(),param_path )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "xmiqqEasYfmm",
        "outputId": "886f3fa1-0f87-483c-cc5e-f875255e46f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'new_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-782ffb7fd6bf>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparam_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/saved_parameters/muhsroomIDweights.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmyshroom_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyshroom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_path\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'new_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "01cjhL80qwH9"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}